{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simple LoRA: Low-Rank Adaptation Linear Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Windows\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x1fa66371350>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import random\n",
    "import math\n",
    "\n",
    "torch.manual_seed(1234)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LoRALayer\n",
    "\n",
    "Create the LoRALayer as discussed in the paper [LoRA: Low-Rank Adaptation of Large Language Models](https://arxiv.org/abs/2106.09685) by Edward J. Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen, 2023, arXiv.  Also see the 'official LoRA' github at [GitHub:microsoft/LoRA](https://github.com/microsoft/LoRA/tree/main) by Microsoft Corporation, 2023.\n",
    "\n",
    "The basic idea is to add the LoRA output to the original Linear layer output as shown below with 'x + x1', where 'x1 = (x @ A @ B) * scaling'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LoRALayer(torch.nn.Module):\n",
    "    def __init__(self, in_dim, out_dim, rank, alpha):\n",
    "        super().__init__()\n",
    "        # Initialize A with random values from N(0, 1/sqrt(rank))\n",
    "        stdev = torch.tensor(1.0 / math.sqrt(rank))\n",
    "        self.A = torch.nn.Parameter(torch.randn(in_dim, rank) * stdev)\n",
    "        # Initialize B with zeros\n",
    "        self.B = torch.nn.Parameter(torch.zeros(rank, out_dim))\n",
    "        self.alpha = alpha\n",
    "        self.scaling = alpha / rank\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x1 = torch.matmul(x, self.A)    # x @ A\n",
    "        x1 = torch.matmul(x1, self.B)   # x @ A @ B\n",
    "        x1 = x1 * self.scaling            # x @ A @ B * scaling\n",
    "        return x + x1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LinearLoRA\n",
    "\n",
    "Next, lets create the LinearLoRA layer that combines a Linear layer with the LoRALayer above.  When using LoRA, the Linear layer\n",
    "weights and bias are 'frozen' by setting their 'requires_grad' = False."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearLoRA(torch.nn.Module):\n",
    "    def __init__(self, in_dim, out_dim, rank, alpha, enable_lora):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.enable_lora = enable_lora       \n",
    "        if enable_lora:\n",
    "            # freeze linear learning when using lora\n",
    "            self.weight = torch.nn.Parameter(torch.randn(in_dim, out_dim), requires_grad=False)\n",
    "            self.bias = torch.nn.Parameter(torch.zeros(out_dim), requires_grad=False)          \n",
    "            self.lora = LoRALayer(in_dim, out_dim, rank, alpha)\n",
    "        else:\n",
    "            self.weight = torch.nn.Parameter(torch.randn(in_dim, out_dim))\n",
    "            self.bias = torch.nn.Parameter(torch.zeros(out_dim))\n",
    "            \n",
    "    def forward(self, x):\n",
    "        original_shape = x.shape\n",
    "        # flatten input\n",
    "        x = x.view(-1, original_shape[-1])\n",
    "        # linear layer\n",
    "        x1 = F.linear(x, self.weight, self.bias)\n",
    "        # lora layer\n",
    "        x2 = self.lora(x1) if self.enable_lora else x1           \n",
    "        # reshape output\n",
    "        x = x2.view(original_shape)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the forward pass, we first flatten the input so that the Linear runs on just the last axis of data.  The Linear layer output 'x1' \n",
    "is passed to the LoRALayer which runs LoRA on 'x1' and then internally adds the results to 'x1'.\n",
    "\n",
    "## Running with LoRA Disabled\n",
    "\n",
    "Now lets try it out.  First we will define the LinearLoRA with LoRA disabled."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LoRA Disabled - start linear weights:  Parameter containing:\n",
      "tensor([[-0.0098]], requires_grad=True)\n",
      "LoRA Disabled - start linear bias:  Parameter containing:\n",
      "tensor([0.], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "input_dim = 1\n",
    "output_dim = 1\n",
    "\n",
    "linear_layer = LinearLoRA(input_dim, output_dim, rank=4, alpha=0.5, enable_lora=False)\n",
    "loss_fn = nn.MSELoss()\n",
    "optimizer = optim.SGD(linear_layer.parameters(), lr=0.01)\n",
    "\n",
    "x_train = torch.tensor([[[1.0], [2.0], [3.0]], [[4.0], [5.0], [6.0]]])\n",
    "y_train = torch.tensor([[[2.0], [4.0], [6.0]], [[8.0], [10.0], [12.0]]])\n",
    "\n",
    "print(\"LoRA Disabled - start linear weights: \", linear_layer.weight)\n",
    "print(\"LoRA Disabled - start linear bias: \", linear_layer.bias)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note the initial linear weight and bias values, which should change when 'enable_lora=False'.  \n",
    "\n",
    "Lets now train the simple linear layer to output [[[2.0], [4.0], [6.0]], [[8.0], [10.0], [12.0]]] when we input [[[1.0], [2.0], [3.0]], [[4.0], [5.0], [6.0]]]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The predicted values are:  tensor([[[ 2.0000],\n",
      "         [ 4.0000],\n",
      "         [ 6.0000]],\n",
      "\n",
      "        [[ 8.0000],\n",
      "         [10.0000],\n",
      "         [12.0000]]], grad_fn=<ViewBackward0>)\n",
      "LoRA Disabled - end linear weights:  Parameter containing:\n",
      "tensor([[-1.4473]])\n",
      "LoRA Disabled - end linear bias:  Parameter containing:\n",
      "tensor([0.])\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(1000):\n",
    "    # Forward pass\n",
    "    y_pred = linear_layer(x_train)\n",
    "\n",
    "    # Compute the loss\n",
    "    loss = loss_fn(y_pred, y_train)\n",
    "\n",
    "    # Zero the gradients\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    # Backward pass\n",
    "    loss.backward()\n",
    "\n",
    "    # Update the weights\n",
    "    optimizer.step()\n",
    "\n",
    "# Test the model\n",
    "x_test = torch.tensor([[[1.0], [2.0], [3.0]], [[4.0], [5.0], [6.0]]])\n",
    "y_test = linear_layer(x_test)\n",
    "print(\"The predicted values are: \", y_test)\n",
    "print(\"LoRA Disabled - end linear weights: \", linear_layer.weight)\n",
    "print(\"LoRA Disabled - end linear bias: \", linear_layer.bias)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note, how the linear weight and bias have changed as they now store the knowlege on how to convert the input to the desired output.\n",
    "\n",
    "## Running with LoRA Enabled\n",
    "\n",
    "Next, lets run the same test, but this time with LoRA turned on (e.g., 'enable_lora' = True).  When enabled, the Linear weights and bias shold not change as they are 'frozen'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LoRA Enabled - start linear weights:  Parameter containing:\n",
      "tensor([[-0.7649]])\n",
      "LoRA Enabled - start linear bias:  Parameter containing:\n",
      "tensor([0.])\n"
     ]
    }
   ],
   "source": [
    "input_dim = 1\n",
    "output_dim = 1\n",
    "\n",
    "linear_layer = LinearLoRA(input_dim, output_dim, rank=4, alpha=0.5, enable_lora=True)\n",
    "loss_fn = nn.MSELoss()\n",
    "optimizer = optim.SGD(linear_layer.parameters(), lr=0.01)\n",
    "\n",
    "x_train = torch.tensor([[[1.0], [2.0], [3.0]], [[4.0], [5.0], [6.0]]])\n",
    "y_train = torch.tensor([[[2.0], [4.0], [6.0]], [[8.0], [10.0], [12.0]]])\n",
    "\n",
    "print(\"LoRA Enabled - start linear weights: \", linear_layer.weight)\n",
    "print(\"LoRA Enabled - start linear bias: \", linear_layer.bias)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, note the initial linear weight and bias values, which should NOT change when 'enable_lora=True', for they are 'frozen'.\n",
    "\n",
    "Lets now again train the simple linear layer to output [[[2.0], [4.0], [6.0]], [[8.0], [10.0], [12.0]]] when we input [[[1.0], [2.0], [3.0]], [[4.0], [5.0], [6.0]]] but this time with LoRA enabled."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The predicted values are:  tensor([[[ 2.0000],\n",
      "         [ 4.0000],\n",
      "         [ 6.0000]],\n",
      "\n",
      "        [[ 8.0000],\n",
      "         [10.0000],\n",
      "         [12.0000]]], grad_fn=<ViewBackward0>)\n",
      "LoRA Enabled - end linear weights:  Parameter containing:\n",
      "tensor([[-0.7649]])\n",
      "LoRA Enabled - end linear bias:  Parameter containing:\n",
      "tensor([0.])\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(1000):\n",
    "    # Forward pass\n",
    "    y_pred = linear_layer(x_train)\n",
    "\n",
    "    # Compute the loss\n",
    "    loss = loss_fn(y_pred, y_train)\n",
    "\n",
    "    # Zero the gradients\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    # Backward pass\n",
    "    loss.backward()\n",
    "\n",
    "    # Update the weights\n",
    "    optimizer.step()\n",
    "\n",
    "# Test the model\n",
    "x_test = torch.tensor([[[1.0], [2.0], [3.0]], [[4.0], [5.0], [6.0]]])\n",
    "y_test = linear_layer(x_test)\n",
    "print(\"The predicted values are: \", y_test)\n",
    "print(\"LoRA Enabled - end linear weights: \", linear_layer.weight)\n",
    "print(\"LoRA Enabled - end linear bias: \", linear_layer.bias)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This time, the linear weights and bias have **not changed**, but the model still learned to output the desired results.  When using LoRA the newly learned knowledge is stored in the LoRA A and B weights instead of the Linear weight and bias.\n",
    "\n",
    "For other LoRA resources see the following.\n",
    "[Code LoRA From Scratch](https://lightning.ai/lightning-ai/studios/code-lora-from-scratch) by Sebastian Raschka, 2023, Lightning.AI\n",
    "[GitHub:minLoRA/cccntu](https://github.com/cccntu/minLoRA) by Jonathan Chen, 2023, GitHub\n",
    "\n",
    "And for a visual description on how LoRA works, see [Understanding LLM Fine Tuning with Low-Rank Adaptation (LoRA)](https://www.signalpop.com/2024/01/28/understanding-llm-fine-tuning-with-low-rank-adaptation-lora/) by SignalPop, 2023."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
